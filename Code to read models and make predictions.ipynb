{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "453a8066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "plt.tight_layout()\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc, recall_score, fbeta_score, precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import chi2_contingency\n",
    "import math\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd0f21",
   "metadata": {},
   "source": [
    "# Loading the prediction data, the models and other important data\n",
    "\n",
    "<span style=\"color: red\">Replace the predict_url and column_dictionary_url variables with the urls of the data you want to make predictions with</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8804e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_url='InputData.csv' #Replace the predict_url variable with the url of the data you want to make predictions with\n",
    "data_predict=pd.read_csv(predict_url)\n",
    "data_predict=data_predict[data_predict['label']=='oot']\n",
    "\n",
    "column_dictionary_url='data_dictionary_v1.xlsx' #Replace the column_dictionary_url variable with the url of the dictionary you want to make predictions with\n",
    "dictionary=pd.read_excel(column_dictionary_url).set_index('Rename').to_dict()\n",
    "\n",
    " #Loading the models and some important data that'll be used for the predictions (derived during the training process)\n",
    "\n",
    "lr_model = pickle.load(open('Default_Predict_LR_model.sav', 'rb'))\n",
    "decision_threshold_lr=0.470235\n",
    "binsDict={'# NEGATIVE EVENTS IN LAST 6 MONTHS (LOANS)': [-0.01, 1, 3, 5, float(\"inf\")],\n",
    "              'CUMULATIVE MINIMUM BALANCE LAST 90 DAYS': [-0.01, 150, 450, 18300, float(\"inf\")],\n",
    "              'gms_version': [-0.01, 203915, 204516, 204713, 204714, float(\"inf\")],\n",
    "              'gms_sub_version': [-0.01, 100406, 120408, float(\"inf\")],\n",
    "              'AVG. OF MINIMUM BALANCE PER MONTH LAST 360 DAYS': [-0.01, 70, 700, 1800, 3600, 13000, 50000, float(\"inf\")],\n",
    "              'count_debit_transactions_last_360_days': [-0.01, 70, 85, 100, 130, 250, float(\"inf\")],\n",
    "              'AVG. MONTHLY DEBIT LAST 30 DAYS BY AVG. MONTHLY DEBIT LAST 90 DAYS': [-0.01, 1.7, 1.88, 2.15, 3, float(\"inf\")],\n",
    "              '# NEFT/RTGS/IMPS TRANSACTIONS LAST 360 DAYS': [-0.01, 2, 5, 15, float(\"inf\")],\n",
    "             'Last closing balance amount (overall)': [-float(\"inf\"), 100, 3700, 7500, 13000, 35000, float(\"inf\")],\n",
    "             '# ATM TRANSACTIONS LIFETIME': [-0.01, 3, 8, 13, 33, float(\"inf\")],\n",
    "             'CURRENT LOAN LIABILITY IN THE LAST 3 MONTHS': [-float(\"inf\"), 40, 1600, 15000, 30000, 80000, float(\"inf\")],\n",
    "             'AVG. DAILY DEBIT LAST 30 DAYS BY AVG. DAILY DEBIT LAST 60-120 DAYS': [-0.01, 1.5, 2, 5, float(\"inf\")],\n",
    "             'AVG. MONTHLY CREDIT TRANSACTIONS AMOUNT LIFETIME': [-0.01, 7000, 13000, 20000, 32000, 125000, 200000, 375000, float(\"inf\")],\n",
    "             'AVG. DAILY DEBIT TRANSACTIONS COUNT LIFETIME': [-0.01, 0.27, 0.43, 0.65, 3.28, float(\"inf\")],\n",
    "             'TOTAL DEBIT AMOUNT : TOTAL CREDIT AMOUNT RATIO LAST 90 DAYS': [-0.01, 0.96, 1, 1.02, 1.36, float(\"inf\")],\n",
    "             'distance_from_pin_code': [0, 2000, 13500, 150000, float(\"inf\")],\n",
    "             '# Loan defaults in last 21 days': [-0.01, 1, 2, float(\"inf\")],\n",
    "             'AVG. MISSED PAYMENT AMOUNT LAST 360 DAYS': [-float(\"inf\"), 1300, 7500, float(\"inf\")]\n",
    "                 }\n",
    "binData=pd.read_csv('binData.csv')\n",
    "binData.set_index(['column', 'bins'], inplace=True)\n",
    "\n",
    "\n",
    "rf_model = pickle.load(open('Default_Predict_RF_model.sav', 'rb'))\n",
    "decision_threshold_rf=0.571695\n",
    "RF_fit_Columns=['AVG. DAILY DEBIT TRANSACTIONS COUNT LIFETIME',\n",
    " 'AVG. DAILY DEBIT TRANSACTIONS AMOUNT LAST 180 DAYS',\n",
    " 'AVG. OF MINIMUM BALANCE PER MONTH LAST 360 DAYS',\n",
    " 'count_debit_transactions_last_360_days',\n",
    " 'AVG. CREDIT PER TRANSACTION LAST 30 DAYS',\n",
    " 'AVG. MONTHLY DEBIT CARD TRANSACTIONS AMOUNT LAST 180 DAYS',\n",
    " 'AVG. MONTHLY DEBIT LAST 30 DAYS BY AVG. MONTHLY DEBIT LAST 90 DAYS',\n",
    " 'gms_sub_version',\n",
    " 'AVG. MONTHLY CREDIT TRANSACTIONS AMOUNT LAST 180 DAYS',\n",
    " 'CURRENT LOAN LIABILITY IN THE LAST 3 MONTHS',\n",
    " 'AVG. OF MAXIMUM BALANCE PER MONTH LIFETIME',\n",
    " 'AVG. MISSED PAYMENT AMOUNT LAST 360 DAYS',\n",
    " 'total_debit_transaction_amount_last_90_days',\n",
    " 'AVG. DAILY DEBIT LAST 30 DAYS BY AVG. DAILY DEBIT LAST 60-120 DAYS',\n",
    " 'CUMULATIVE MAXIMUM BALANCE LAST 30 DAYS',\n",
    " '# UTILITIES PAYING BILLS FOR',\n",
    " 'AVG. MONTHLY CREDIT TRANSACTIONS AMOUNT LIFETIME',\n",
    " 'gms_version']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da69fa",
   "metadata": {},
   "source": [
    "# Defining the classes for the Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd51d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Cleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dictionary): # no *args or *kargs\n",
    "        self.dictionary=dictionary\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "    #Replacing the dummy column names with actual column names from the provided dictionary. This will help in our analysis.\n",
    "        X.rename(columns = dictionary['description'], inplace = True)\n",
    "        X.drop(columns = ['label', 'brand', 'carrier', 'device_name', 'manufacturer', 'network_type', 'screen_height', 'screen_width', 'screen_dpi'], inplace = True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "698bd169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class woe_out_of_sample(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, binDict, binData): # no *args or *kargs\n",
    "        self.binDict=binDict\n",
    "        self.binData=binData\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        return self\n",
    "    def transform(self, X): \n",
    "        cols=X.columns\n",
    "        for col in cols[~cols.isin(['default'])]: \n",
    "            X[col]=pd.cut(X[col], bins=self.binDict[col])\n",
    "            X[col]=(X[col].cat.add_categories('missing values').fillna('missing values'))\n",
    "            X[col]=X[col].astype(\"str\")\n",
    "            X[col]=X[col].apply(lambda x, col=col, binData=self.binData: binData.loc[col, x])\n",
    "#         print(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6217854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class columnFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or *kargs\n",
    "        pass\n",
    "    def process_gms_sub_version(self, gms: str):\n",
    "#     print(gms)\n",
    "        if(gms!=gms):\n",
    "            return gms\n",
    "        else:\n",
    "            return(float((gms.split()[1][1:]).split('-')[0]))\n",
    "    def process_gms_version(self, gms: str):\n",
    "        if(gms!=gms):\n",
    "            return gms\n",
    "        else:\n",
    "            split=re.split(',|\\.', gms.split(' ')[0])\n",
    "            res=''\n",
    "            for i in split:\n",
    "                if(len(i)==1):\n",
    "                    res=res+'0'+i\n",
    "                else:\n",
    "                    res=res+i\n",
    "            return(float(res))\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X['gms_sub_version']= X['gms_version'].apply(self.process_gms_sub_version)\n",
    "        X['gms_version'] = X['gms_version'].apply(self.process_gms_version)\n",
    "#         print(X)\n",
    "        return(X.loc[:,['default', 'gms_version', 'gms_sub_version', '# NEGATIVE EVENTS IN LAST 6 MONTHS (LOANS)', 'CUMULATIVE MINIMUM BALANCE LAST 90 DAYS', 'AVG. OF MINIMUM BALANCE PER MONTH LAST 360 DAYS', 'count_debit_transactions_last_360_days', 'Last closing balance amount (overall)', '# ATM TRANSACTIONS LIFETIME', 'AVG. MONTHLY DEBIT LAST 30 DAYS BY AVG. MONTHLY DEBIT LAST 90 DAYS', '# NEFT/RTGS/IMPS TRANSACTIONS LAST 360 DAYS', 'CURRENT LOAN LIABILITY IN THE LAST 3 MONTHS', 'AVG. DAILY DEBIT LAST 30 DAYS BY AVG. DAILY DEBIT LAST 60-120 DAYS',  'AVG. MONTHLY CREDIT TRANSACTIONS AMOUNT LIFETIME', 'AVG. DAILY DEBIT TRANSACTIONS COUNT LIFETIME', 'TOTAL DEBIT AMOUNT : TOTAL CREDIT AMOUNT RATIO LAST 90 DAYS', 'distance_from_pin_code', '# Loan defaults in last 21 days', 'AVG. MISSED PAYMENT AMOUNT LAST 360 DAYS']])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6df0d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering transformer to create the final 'gms_version' and 'gms_sub_version' features\n",
    "\n",
    "class RF_Preprocessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or *kargs\n",
    "        pass\n",
    "    def process_gms_sub_version(self, gms: str):\n",
    "    #     print(gms)\n",
    "        if(gms!=gms):\n",
    "            return gms\n",
    "        else:\n",
    "            return(float((gms.split()[1][1:]).split('-')[0]))\n",
    "    def process_gms_version(self, gms: str):\n",
    "#     print(gms)\n",
    "        if(gms!=gms):\n",
    "            return gms\n",
    "        else:\n",
    "            split=re.split(',|\\.', gms.split(' ')[0])\n",
    "            res=''\n",
    "            for i in split:\n",
    "                if(len(i)==1):\n",
    "                    res=res+'0'+i\n",
    "                else:\n",
    "                    res=res+i\n",
    "            return(float(res))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X['gms_sub_version']= X['gms_version'].apply(self.process_gms_sub_version)\n",
    "        X['gms_version'] = X['gms_version'].apply(self.process_gms_version)\n",
    "        X=X.fillna(-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04919512",
   "metadata": {},
   "source": [
    "# Transforming the prediction data using the pipelines and making the final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68341810",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_ids=data_predict.copy()['decision_id']\n",
    "\n",
    "woe_oos = woe_out_of_sample(binsDict, binData)\n",
    "filterColumns=columnFilter()\n",
    "dataCleaner=Data_Cleaner(dictionary)\n",
    "pipeline_predict_lr = Pipeline(steps=[('cleanData', dataCleaner), ('filter', filterColumns), ('woe_predict', woe_oos)])\n",
    "transformed_predict_Data_LR=pipeline_predict_lr.fit_transform(data_predict.copy().drop(columns=['decision_id']))\n",
    "X_predict_LR=transformed_predict_Data_LR.drop(columns=['default'])\n",
    "y_pred_proba_LR_predict=pd.DataFrame(lr_model.predict_proba(X_predict_LR), index=decision_ids.index)\n",
    "y_pred_adjusted_LR_predict=y_pred_proba_LR_predict.apply(lambda x: 0 if x[1]<decision_threshold_lr else 1, axis=1)\n",
    "y_pred_adjusted_LR_predict=pd.concat([decision_ids, y_pred_adjusted_LR_predict], axis=1).rename(columns={0: 'default'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "06b641a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preprocessor=RF_Preprocessing()\n",
    "testRFPipeline=Pipeline([('cleanData', dataCleaner), ('preprocessing', rf_preprocessor)])\n",
    "transformed_predict_Data_RF=testRFPipeline.fit_transform(data_predict.copy().drop(columns=['default', 'decision_id']))\n",
    "X_predict_RF=transformed_predict_Data_RF\n",
    "X_predict_RF=X_predict_RF.loc[:,RF_fit_Columns]\n",
    "y_pred_proba_RF_predict=pd.DataFrame(rf_model.predict_proba(X_predict_RF), index=decision_ids.index)\n",
    "y_pred_adjusted_RF_predict=y_pred_proba_RF_predict.apply(lambda x: 0 if x[1]<decision_threshold_rf else 1, axis=1)\n",
    "y_pred_adjusted_RF_predict=pd.concat([decision_ids, y_pred_adjusted_RF_predict], axis=1).rename(columns={0: 'default'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf84cbd",
   "metadata": {},
   "source": [
    "### The variables y_pred_adjusted_LR_predict and y_pred_adjusted_RF_predict contain the final predictions for the two models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
